---
title: "AI for Mathematics and Optimization"

# author:
#   - name: "Niao He"
#   - name: "Zebang Shen"

# affiliations:
#   - name: "YOUR INSTITUTE NAME HERE"


author:
  - name: Niao He
    affiliation: 
      - ETH Zurich
  - name: Zebang Shen
    affiliation: ETH Zurich
---


## AI as a tool
::: {style="font-size: 0.8em;"}

  - AI accelerates scientific research.
    - Examples in Phyics and Biology: ([AI for Science Seminars in the past](https://odi.inf.ethz.ch/teaching/AI4Science.html))

      :::: {.columns}
      ::: {.column width="50%"}
      ![Physics-informed Neural Network](elements/PINN.png){.cap-center fig-align="center" height=240px}
      :::
      ::: {.column width="50%"}
      ![AlphaFold 2](elements/AlphaFold_2.png){.cap-center fig-align="center" height=240px}
      :::
      ::::
  - How does AI help? 
    - Function approximation, simulation, routine automation, etc.
    
:::

## AI as a collaborator
:::: {.columns}

::: {.column width="50%" style="font-size: 0.8em;"}

- Focus: AI accelerates mathematical research.
  - Core intellecture challenge
    - formulating hypothesis
    - design algorithms
    - proving theorems
    - constructing counterexamples


:::

::: {.column width="50%"}

::: {.v-center-60}

![](elements/nature_2021.png){.cap-center fig-align="center" height=400px}

:::

:::
::::

## Example: First Proof

<div style="height: 0.8em;"></div>

<iframe
  src="https://1stproof.org/"
  style="width:100%; height:50vh; border:0;"
  title="Example website">
</iframe>

## Example: First Proof (cont.) 

<div style="height: 0.8em;"></div>


:::: {.columns}
::: {.column width="50%"}
![X post](elements/first-proof-attempt.png){.cap-center fig-align="center" height=400px}
:::
::: {.column width="50%"}
![scientific american](elements/first-proof-report.png){.cap-center fig-align="center" height=400px}
:::
::::




## Course organization
- _Passive_ in previous semester:
  - Students are assigned with published papers.
  - Present the existing results in the paper in the end of the semester
- _Active_ in this semester:
  - Students are assigned with **open problems** with the ambition to solve it!
  - If succeed, lead to technical report or even **publications**.
    <!-- - More to be covered later. -->

## Part of a social experiment
- How AI helps students to study new/abstract/challenging materials? 
- How AI helps students to solve open problems?

## Why can we expect this is achievable?
- Emergence of LLM, success in mathematical competition.
  
  ![](elements/ai_math_progress_math_benchmark.png){fig-align="center" height=300px}

- Many evidentce showing the potential of LLM in mathematical research
  <!-- - Inspiration/Prompting techniques from the success stories -->

## A Long List

<div style="height: 0.8em;"></div>

<iframe
  src="https://seewoo5.github.io/awesome-ai-for-math/?utm_source=chatgpt.com"
  style="width:100%; height:50vh; border:0;"
  title="Example website">
</iframe>

## Elliptic Curve ‘Murmurations’ (Hypothesis) {style="font-size: 0.8em;"}

:::: {.columns .bleed}

::: {.column width="30%"}
![](elements/Elliptic-curve.png){.cap-center fig-align="center"}
:::

::: {.column width="90%" style="font-size: 0.6em;"}

- Elliptic curves are crucial in number theory
  - A key part of Andrew Wiles’ famous proof for Fermat’s Last Theorem.
  - Crucial in Cryptography
  - BSD conjecture (a Clay Millennium Prize Problem) is about a curve’s `rank`
- AI can (magically) accurately predict the `rank` of the elliptic curve, but why?
  - A wave type relation between prime number and the number of solution to the elliptic curve is observed statistically. Suggest hidden structure.
  - Progress toward “explaining why”.
    <div style="height: 0.3em;"></div>
    ![](elements/flocking_proof.png){.cap-center fig-align="center"}
<!-- ::: {.v-center-60} -->
  <!-- <video controls width="720">
    <source src="https://www.quantamagazine.org/wp-content/uploads/2024/03/Murmurations-byPaulChaikin-Lede.mp4" type="video/mp4">
  </video> -->
<!-- ::: -->

:::
::::

## Alphaevolve and faster $4\times 4$ matrix multiplication (algorithm) {style="font-size: 0.6em;"}

<div style="height: 0.8em;"></div>

:::: {.columns}

::: {.column width="40%"}
![](elements/alphaevolve-matrix-multiplication-evolution.webp){.cap-center fig-align="center"}
:::

::: {.column width="60%"}
- Matrix multiplications are building blocks of modern age.
- Key performance metric is (# scalar multiplications).
- AlphaEvolve improves over existing method from 49 to 48, in terms of (# scalar multiplications)
  - sets up matrix multiplication as a searchable optimization problem
  - starts from a known solver, then evolves the solver code
  - loop: propose → run → score → keep best → repeat
- Scales to multiplication in larger sizes if called recursively.

:::

::::


<!-- ## Success stories (counterexample) -->
## Reports from Big Companies {auto-animate=true}

<div style="height: 0.8em;"></div>

:::: {.columns}
::: {.column width="50%"}
![Google](elements/Gemini-paper.png){.cap-center fig-align="center"}
:::
::: {.column width="50%"}
![Open AI](elements/Openai-paper.png){.cap-center fig-align="center"}
:::
::::

## Success stories (counterexample) {auto-animate=true}

<div style="height: 0.8em;"></div>

:::: {.columns}
::: {.column width="30%"}
![Google](elements/Gemini-paper.png){.cap-center fig-align="center"}
:::
::: {.column width="70%"}
![](elements/conjecture.png){.cap-center fig-align="center"}
<div style="height: 0.8em;"></div>
![](elements/counterexample.png){.cap-center fig-align="center"}
:::
::::

## Last iterate convergence (theorem) {auto-animate=true}

:::: {.columns .stack}
::: {.column width="50%" style="font-size: 0.7em;"}
- Convex optimization problem
  $$
    \min_x f(x)
  $$
- Gradient Descent, GD (Cauchy, 18th century)
  $$
    z_{k+1} : = z_k - \eta \nabla f(z_k)
  $$
- Nesterov's Accelerated Gradient Descent, NAG (Nesterov, 1983)
  $$
  \begin{aligned}
    x_{k+1} : =&\ y_k - \eta\ \nabla f(y_k)\\
    y_{k+1} : =&\ x_{k+1} + \frac{t_k - 1}{t_{k+1}} (x_{k+1} - x_k)
  \end{aligned}
  $$
:::
::: {.column width="50%" style="font-size: 0.7em;"}
- Question: $\lim_{k\to \infty} x_k \in \mathrm{argmin}\ f$?


::: {.v-center-30}
![](elements/last-point-convergence.png){.cap-center fig-align="center"}
:::
:::
::::

  
    

## Concrete Example {auto-animate=true}

::: {.columns}
::: {.column width="50%" style="font-size: 0.7em;"}
- Convex optimization problem
  $$
    f(x_1, x_2) = \frac{x_1^2 + (|x_2+1|-1)_+^2}{2}
  $$
- GD (slow convergence, convergent)
  $$
  \begin{aligned}
  f(z_{k}) = \mathcal{O}(\frac{1}{k}), \\
    \lim_{k\to \infty} z_k \in \mathrm{argmin} f
  \end{aligned}
  $$
- NAG (fast convergence, oscillation)
  $$
  \begin{aligned}
  f(x_{k}) = \mathcal{O}(\frac{1}{k^2}), \\
    \lim_{k\to \infty} x_k \in \mathrm{argmin}\ f\ ?
  \end{aligned}
  $$
:::

::: {.column width="50%"}

::: {.v-center-60}
<div style="height: 3em;"></div>
  <video controls width="720">
    <source src="elements/gd_vs_nag_3d.mp4" type="video/mp4">
  </video>
:::

:::

:::

## Stratgy {style="font-size: 0.7em;"}

- Continuous time model, Nesterov ODE (Su, Boyd, Candès, 2016)
  $$
    \ddot X(t) + \frac{\gamma}{t} \dot X(t) + \nabla f(X(t)) = 0.
  $$
- Known results
  - $\gamma > 3$, we have $\lim_{t \to \infty} X(t) = X(\infty) \in \mathrm{argmin} f.$ 
    - Rely on the integrability of an energy function.
  - $\gamma = 3$ critical case, corresponds to NAG. 
    - The integral of the energy function in the $\gamma > 3$ diverges.
- Strategy: Ask query AI to propose a new energy function that is integrable.
  - Once the continous time model converges, same idea applies to the discrete time dynamics.

## Interaction with AI
<div style="height: 0.8em;"></div>

Quote from (Jang and Ryu, 2025):

![](elements/interaction_with_AI.png){.cap-center fig-align="center"}



<!-- ## Open Problem Collections -->


